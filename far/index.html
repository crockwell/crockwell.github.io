
<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="FAR: Flexible, Accurate and Robust 6DoF Relative Camera Pose Estimation">
  <meta name="keywords" content="FAR, Camera Pose Estimation, 3D Understanding, Metric Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>FAR: Flexible, Accurate and Robust 6DoF Relative Camera Pose Estimation</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://crockwell.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="http://vision.eecs.umich.edu">
            UMich Vision Lab
          </a>
          <a class="navbar-item" href="https://crockwell.github.io/rel_pose/">
            8-Point ViT
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">FAR: Flexible, Accurate and Robust 6DoF Relative Camera Pose Estimation</h1>
          <h2 class="title is-5 publication-title">CVPR 2024</h2>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://crockwell.github.io/">Chris Rockwell</a><sup>1</sup></span>,
            </span>
            <span class="author-block">
              <a href="https://nileshkulkarni.github.io/">Nilesh Kulkarni</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://jinlinyi.github.io/">Linyi Jin</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://jjparkcv.github.io/">JJ Park</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://web.eecs.umich.edu/~justincj/">Justin Johnson</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://cs.nyu.edu/~fouhey/">David F. Fouhey</a><sup>2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Michigan,</span>
            <span class="author-block"><sup>2</sup>New York University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link (Original) -->
              <span class="link-block">
                <a href="./data/paper.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- arXiv Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2403.03221"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. 
              <span class="link-block">
                <a href="https://youtu.be/fjRnFL91EZc"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/crockwell/far"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

  

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <img src="./data/teaser.png" alt="FAR Teaser">
        <h2 class="subtitle has-text-justified">
          Figure 1. <b>Precise and robust 6DoF pose estimation.</b> Correspondence Estimation + Solver methods (here LoFTR, RANSAC) produce precise outputs 
          for moderate rotations, but are not robust to large rotations (left), and cannot produce 
          translation scale. Learning-based methods (here LoFTR with 8-Point ViT head) produce scale (right) and are more robust, 
          but lack precision (left). FAR leverages both for precise and robust prediction, including scale.
        </h2>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Estimating relative camera poses between images has been a central problem in computer vision. Methods that
            find correspondences and solve for the fundamental matrix
            offer high precision in most cases. Conversely, methods predicting pose directly using neural networks are more robust
            to limited overlap and can infer absolute translation scale,
            but at the expense of reduced precision. We show how to
            combine the best of both methods; our approach yields results that are both precise and robust, while also accurately
            inferring translation scales. At the heart of our model lies
            a Transformer that (1) learns to balance between solved
            and learned pose estimations, and (2) provides a prior to
            guide a solver. A comprehensive analysis supports our design choices and demonstrates that our method adapts flexibly to various feature extractors and correspondence estimators, showing state-of-the-art performance in 6DoF pose
            estimation on Matterport3D, InteriorNet, StreetLearn, and
            Map-Free Relocalization.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
      
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">Approach</h2>
        <img src="./data/approach.png" alt="FAR Approach">
        <h2 class="subtitle has-text-justified">
          Figure 3. <b>Overview.</b> Given dense features and correspondences, FAR’s Transformer produces camera poses (in square boxes <span style="background-color: #E69F00">&#9633;</span>) through
          a transformer (round box <span style="background-color: #E69F00">&#9634;</span>) and classical solver (round box <span style="background-color: #57B4E9">&#9634;</span>). 
          In the first round, the solver produces a pose <span style="background-color: #57B4E9">T<sub>s</sub></span>. FAR’s pose transformer
          averages this with its own prediction <span style="background-color: #E69F00">T<sub>t</sub></span> via weight <span style="background-color: #019E73">w</span>, 
          to yield the round 1 pose <span style="background-color: #F0E442">T<sub>1</sub></span>. 
          <span style="background-color: #F0E442">T<sub>1</sub></span> pose serves as a prior for the classic solver,
          which produces an updated pose <span style="background-color: #BB4D89">T<sub>u</sub></span>. 
          This is combined with an additional estimate of <span style="background-color: #E69F00">T<sub>t</sub></span> and weight <span style="background-color: #019E73">w</span> 
          to produce the final result <span style="background-color: #019E73">T</span>.
          With few correspondences, <span style="background-color: #F0E442">T<sub>1</sub></span> helps solver output, while the network learns to weigh Transformer 
          predictions more heavily; with many correspondences, solver output is often good, so the network relies mostly on solver output.
        </h2>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{Rockwell2024,
        author = {Rockwell, Chris and Kulkarni, Nilesh and Jin, Linyi and Park, Jeong Joon and Johnson, Justin and Fouhey, David F.},
        title = {FAR: Flexible, Accurate and Robust 6DoF Relative Camera Pose Estimation},
        booktitle = {CVPR},
        year = 2024
    }</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./data/paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/crockwell/far" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Website source code borrowed from <a
              href="https://github.com/nerfies/nerfies.github.io">here</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
