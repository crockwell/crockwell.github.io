<html style="--whitebusterR:255; --whitebusterG:250; --whitebusterB:250;" class="whiteBuster ">
  <head>
    <script async="" src="//www.google-analytics.com/analytics.js"></script><script src="http://www.google.com/jsapi" type="text/javascript"></script> 
    <script type="text/javascript">google.load("jquery", "1.3.2");</script>
    
    <style type="text/css">
      body {
          font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
          font-weight:300;
          font-size:20px;
          position: relative;
      }
      
      h1 {
          font-weight:350;
      }
      
      .disclaimerbox {
          background-color: #eee;		
          border: 1px solid #eeeeee;
          border-radius: 10px ;
          -moz-border-radius: 10px ;
          -webkit-border-radius: 10px ;
          padding: 20px;
      }
  
      video {
          display: block;
          margin: 0 auto;
      }
      
      img.header-img {
          height: 140px;
          border: 1px solid black;
          border-radius: 10px ;
          -moz-border-radius: 10px ;
          -webkit-border-radius: 10px ;
      }
      
      img.rounded {
          border: 1px solid #eeeeee;
          border-radius: 10px ;
          -moz-border-radius: 10px ;
          -webkit-border-radius: 10px ;
      }
      
      a:link,a:visited
      {
          color: #1367a7;
          text-decoration: none;
      }
      a:hover {
          color: #208799;
      }
      
      td.dl-link {
          height: 160px;
          text-align: center;
          font-size: 22px;
      }
      
      .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
          box-shadow:
                  0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
                  5px 5px 0 0px #fff, /* The second layer */
                  5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
                  10px 10px 0 0px #fff, /* The third layer */
                  10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
                  15px 15px 0 0px #fff, /* The fourth layer */
                  15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
                  20px 20px 0 0px #fff, /* The fifth layer */
                  20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
                  25px 25px 0 0px #fff, /* The fifth layer */
                  25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
          margin-left: 10px;
          margin-right: 45px;
      }
  
      .paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
          box-shadow:
                  0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */
  
          margin-left: 10px;
          margin-right: 45px;
      }
  
  
      .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
          box-shadow:
                  0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
                  5px 5px 0 0px #fff, /* The second layer */
                  5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
                  10px 10px 0 0px #fff, /* The third layer */
                  10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
          margin-top: 5px;
          margin-left: 10px;
          margin-right: 30px;
          margin-bottom: 5px;
      }
      
      .vert-cent {
          position: relative;
          top: 50%;
          transform: translateY(-50%);
      }
      
      hr {
          border: 0;
          height: 1px;
          background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
      }

      div.section {
        margin-top: 30px;
        margin-bottom: 30px;
        margin-left: 60px;
        margin-right: 60px;
        font-size: 20px;
      }
      div.mini-section {
        text-align: center;
        font-size: 20px;
        margin-bottom: 30px;
      }
      div.subsection {
        font-size: 20px;
      }      
    </style>
    <title>FAR: Flexible, Accurate and Robust 6DoF Relative Camera Pose Estimation</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=yes">
  </head>
  <body>
    <br>
    <center>
      <span style="font-size:46px">FAR: Flexible, Accurate and Robust 6DoF Relative Camera Pose Estimation</span>
      <table align="center" width="600px">
        <tbody>
          <tr>
            <td align="center" width="100px">
              <center><span style="font-size:27px"><a href="https://crockwell.github.io/">Chris Rockwell</a><sup>1</sup></span></center>
            </td>
            <td align="center" width="100px">
              <center><span style="font-size:27px"><a href="https://nileshkulkarni.github.io/">Nilesh Kulkarni</a><sup>1</sup></span></center>
            </td>
            <td align="center" width="100px">
              <center><span style="font-size:27px"><a href="https://jinlinyi.github.io/">Linyi Jin</a><sup>1</sup></span></center>
            </td>
            <td align="center" width="100px">
              <center><span style="font-size:27px"><a href="https://jjparkcv.github.io/">JJ Park</a><sup>1</sup></span></center>
            </td>
            <td align="center" width="100px">
              <center><span style="font-size:27px"><a href="https://web.eecs.umich.edu/~justincj/">Justin Johnson</a><sup>1</sup></span></center>
            </td>
            <td align="center" width="100px">
              <center><span style="font-size:27px"><a href="https://web.eecs.umich.edu/~fouhey/">David F. Fouhey</a><sup>2</sup></span></center>
            </td>
          </tr>
        </tbody>
      </table>
      <table align="center" width="650px">
        <tbody>
          <tr>
            <td align="center" width="100px">
              <center><span style="font-size:27px"><sup>1</sup>University of Michigan</span></center>
            </td>
            <td align="center" width="100px">
              <center><span style="font-size:27px"><sup>2</sup>New York University</span></center>
            </td>
          </tr>
        <tr>
          <tbody>
            <tr>
              <td align="center" width="100px">
                <center><span style="font-size:27px">arXiv 2023</span></center>
              </td>
            </tr>
        </tbody>
      </table>
      <table align="center" width="920px">
        <tbody>
            <td align="center" width="230px">
              <center><span style="font-size:27px"><a href="https://github.com/crockwell/far"> [GitHub]</a></span></center>
            </td>
            <td align="center" width="230px">
              <center><span style="font-size:27px"><a href=""> [arXiv]</a></span></center>
            </td>
            <td align="center" width="230px">
              <center><span style="font-size:27px"><a href="https://crockwell.github.io/far/rockwell2023.html"> [BibTex]</a></span></center>
            </td>
          </tr>
          <tr></tr>
        </tbody>
      </table>
    </center>
    <br>
    <hr style="background-image: linear-gradient(to right, rgba(1, 1, 1, 0), rgba(0, 0, 0, 0.75), rgba(1, 1, 1, 0));">

    <br>
    <div class='section'>
      <a id="teaser"></a>
      <center><a href="./data/teaser.png"></a><img src="./data/teaser.png" height="270px"></a><br> <!--  height="300px" -->
      <center><span style="font-size:14px"> 
        Figure 1. <b>Precise and robust 6DoF pose estimation.</b> Correspondence Estimation + Solver (here LoFTR, RANSAC)
        methods produce precise outputs for moderate rotations, but are not robust to large rotations (left), and cannot produce 
        translation scale. Learning-based methods (here LoFTR with 8-Point ViT [61] head) produce scale (right) and are more robust, 
        but lack precision (left). FAR leverages both for precise and robust prediction, including scale.
      </span></center>
    </div>
    <br>
    <hr style="background-image: linear-gradient(to right, rgba(1, 1, 1, 0), rgba(0, 0, 0, 0.75), rgba(1, 1, 1, 0));">


    <br>
    <div class='section'>
      <a id="abstract"></a>
      <center><h1>Abstract</h1></center><br>
      Estimating relative camera poses between images has been a central problem in computer vision. Methods that
      find correspondences and solve for the fundamental matrix
      offer high precision in most cases. Conversely, methods predicting pose directly using neural networks are more robust
      to limited overlap and can infer absolute translation scale,
      but at the expense of reduced precision. We show how to
      combine the best of both methods; our approach yields results that are both precise and robust, while also accurately
      inferring translation scales. At the heart of our model lies
      a Transformer that (1) learns to balance between solved
      and learned pose estimations, and (2) provides a prior to
      guide a solver. A comprehensive analysis supports our design choices and demonstrates that our method adapts flexibly to various feature extractors and correspondence estimators, showing state-of-the-art performance in 6DoF pose
      estimation on Matterport3D, InteriorNet, StreetLearn, and
      Map-Free Relocalization.
    </div>
    <br>
    <hr style="background-image: linear-gradient(to right, rgba(1, 1, 1, 0), rgba(0, 0, 0, 0.75), rgba(1, 1, 1, 0));">

    <br>
    <div class='section'>
      <center><h1>Approach</h1></center><br>
      <a id="approach"></a>
      <center><a href="./data/approach.png"></a><img src="./data/approach.png" height="240px"></a><br> <!--  height="300px" -->

      <center><span style="font-size:14px"> 
        Figure 3. <b>Overview.</b> Given dense features and correspondences, FAR’s Transformer produces camera poses (in square boxes <span style="background-color: #E69F00">&#9633;</span>) through
        a transformer (round box <span style="background-color: #E69F00">&#9634;</span>) and classical solver (round box <span style="background-color: #57B4E9">&#9634;</span>). 
        In the first round, the solver produces a pose <span style="background-color: #57B4E9">T<sub>s</sub></span>. FAR’s pose transformer
        averages this with its own prediction <span style="background-color: #E69F00">T<sub>t</sub></span> via weight <span style="background-color: #019E73">w</span>, 
        to yield the round 1 pose <span style="background-color: #F0E442">T<sub>1</sub></span>. 
        <span style="background-color: #F0E442">T<sub>1</sub></span> pose serves as a prior for the classic solver,
        which produces an updated pose <span style="background-color: #BB4D89">T<sub>u</sub></span>. 
        This is combined with an additional estimate of <span style="background-color: #E69F00">T<sub>t</sub></span> and weight <span style="background-color: #019E73">w</span> 
        to produce the final result <span style="background-color: #019E73">T</span>.
        With few correspondences, <span style="background-color: #F0E442">T<sub>1</sub></span> helps solver output, while the network learns to weigh Transformer 
        predictions more heavily; with many correspondences, solver output is often good, so the network relies mostly on solver output.
      </span></center>
    </div>
    <br>
    <hr style="background-image: linear-gradient(to right, rgba(1, 1, 1, 0), rgba(0, 0, 0, 0.75), rgba(1, 1, 1, 0));">

    <br>
    <div class='subsection'>
      <center><h1>Paper and Supplemental Material</h1></center><br>
      <div class='row'>
        <div class='col-sm-2'></div>
        <div class='col-sm-4 text-center'> 
          <a href=""><img class="layered-paper-big" style="height:175px" src="./data/preview.png"></a>
        </div>
        <div class='col-sm-4'>
          Rockwell, Kulkarni, Jin, Park, Johnson and Fouhey. <br>
          FAR: Flexible, Accurate and Robust 6DoF Relative Camera Pose Estimation. <br>
          arXiv 2023. (Hosted on <a href="">arXiv</a>)
        </div>
      </div>
    </div>
    <br>
    <br>
    <div class='mini-section'>
      <a href="./data/paper.pdf">[Paper+Supplemental]</a>
      <a href="https://github.com/crockwell/far/">[Code]</a>
    </div>
    <table align="center" width="600px">
      <div>
        <tr>
          <pre class="citation">
            @inProceedings{Rockwell2023,
              author = {Chris Rockwell and Nilesh Kulkarni and Linyi Jin and Jeong Joon Park and Justin Johnson and David F. Fouhey},
              title = {FAR: Flexible, Accurate and Robust 6DoF Relative Camera Pose Estimation},
              booktitle = {arXiv},
              year = 2023
            }
          </pre>
        </tr> 
      </div>
    </table>
    <hr>
    <div class='section'>
      <center><h1>Acknowledgements</h1></center><br>
      Thanks to <a href="https://jeongsoop.github.io/">Jeongsoo Park</a> and <a href="https://sites.google.com/view/sangwoomo">Sangwoo Mo</a> for their helpful feedback. 
      Thanks to Laura Fink and UM DCO for their tireless computing support. 
      The webpage template originally came from some
      <a href="https://richzhang.github.io/colorization/">colorful folks</a>.
    </div>
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    
      ga('create', 'UA-75863369-1', 'auto');
      ga('send', 'pageview');
    </script>
  </body>
</html>
