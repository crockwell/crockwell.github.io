<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Chris Rockwell</title>
  
  <meta name="author" content="Chris Rockwell">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- for Twitter / facebook sharing -- not implemented yet
  <meta property="og:locale" content="en_US">
  <meta property="og:type" content="website">
  <meta property="og:title" content="Karan Desai">
  <meta property="og:url" content="https://kdexd.github.io/">
  <meta property="og:site_name" content="Karan Desai">
  <meta property="og:image" content="https://kdexd.github.io/static/img/kd.jpg"/>
  <meta property="og:image:url" content="https://kdexd.github.io/static/img/kd.jpg"/>
  <meta property="og:description" content="Visiting research scholar at Georgia Tech, IIT Roorkee Alum.">
  <meta name="twitter:card" content="summary_large_image"/>
  <meta name="twitter:title" content="Karan Desai">
  <meta name="twitter:description" content="Visiting research scholar at Georgia Tech, IIT Roorkee Alum.">
  <meta name="twitter:creator" content="@kdexd">

  <meta name="twitter:image" content="https://kdexd.github.io/static/img/kd.jpg"/>
  <meta name="twitter:url" content="https://kdexd.github.io/static/img/kd.jpg"/>
  <meta name="twitter:description" content="Visiting research scholar at Georgia Tech, IIT Roorkee Alum."/>
  <meta name="twitter:domain" value="kdexd.github.io">

  <meta name="twitter:label1" value="Visiting Research Scholar"/>
  <meta name="twitter:data1" value="Georgia Tech"/>
  -->

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-153849999-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-153849999-1');
</script>

<script type="text/javascript">
  var current=1; var total=3; var time;
  function imageChanger() { 
    time=setTimeout("changeImage()", 5000); 
  }
  function changeImage() { 
    current++;
    if(current > total) current=1;
    document.getElementById("profile").src="images/prof_crop_circle"+current+".png";
    imageChanger(); 
  }

</script>

<!-- head -->
<script src="https://tarptaeya.github.io/repo-card/repo-card.js"></script>
</head>

<body onLoad="imageChanger();">
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Chris Rockwell</name>
              </p>
              <p>I am a Ph.D. candidate in Computer Science and Engineering at the <a href="https://www.cse.umich.edu/">University of Michigan</a>.
                I do research in computer vision and machine learning and am advised by
                <a href="https://cs.nyu.edu/~fouhey/">David F. Fouhey</a> and <a href="https://web.eecs.umich.edu/~justincj/">Justin Johnson</a>.
                I have been fortunate to collaborate with others, including: <a href="https://jjparkcv.github.io/">JJ Park</a> and <a href="https://web.eecs.umich.edu/~honglak/">Honglak Lee</a> at UM, 
                <a href="https://jbhuang0604.github.io/">Jia-Bin Huang</a> at Meta Reality Labs
                and <a href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a> at Princeton.
                I'm currently a research intern hosted by <a href="https://chenhsuanlin.bitbucket.io/">Chen-Hsuan Lin</a> at NVIDIA's <a href="https://research.nvidia.com/labs/dir/">Deep Imagination Research group</a>.
                <!--<a href="https://hytseng0509.github.io/">Hung-Yu Tseng</a>, 
                Jia Bin Huang and <a href="http://johanneskopf.de/">Johannes Kopf</a> at <a href="https://tech.fb.com/ar-vr/">Meta Reality Labs</a>. 
                 I also obtained my Master's at the University of Michigan, where I was advised by David Fouhey and 
                <a href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a>. 
                -->
              </p>
              <p> Please feel free to contact me: cnris at umich dot edu
              </p>
              <!--<p> Before grad school, I worked as a trader at <a href="https://www.citadel.com/">Citadel LLC</a>, where I applied statistical arbitrage in fixed income. Earlier, I structured exotic options and created systematic hedging strategies at <a href="https://group.bnpparibas/en/">BNP Paribas</a>. I received my Bachelor's at the University of Michigan, where I researched High Frequency Trading and AI under <a href="https://strategicreasoning.org/michael-p-wellman/"> Michael Wellman</a>. 
              </p>
              <p> <span style="background-color: #FFFF00">I am actively pursuing computer vision research internships for 2024! Please feel free to contact me: cnris at umich dot edu</span>
              </p>
              -->
              
              <p style="text-align:center">
                <a href="https://github.com/crockwell">Github</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=pB9hZ_MAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="data/cv.pdf">CV</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/chris-rockwell-54903b65/"> LinkedIn </a> &nbsp/&nbsp
                <a href="https://twitter.com/_crockwell">Twitter</a>
                
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <img id="profile" style="width:100%;max-width:100%" alt="profile photo" src="images/prof_crop_circle1.png" class="hoverZoomLink">
            </td>
          </tr>
        </tbody></table>
        <!--
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Preprints</heading>
            </td>
          </tr>
        </tbody></table>
        -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
              <a href="images/far_thumbnail.png"><img src="images/far_thumbnail.png" alt="net1" width="100%"></a>
            </td>
            <td width="65%" valign="middle">
              <papertitle><a href="https://crockwell.github.io/far/">FAR: Flexible, Accurate and Robust 6DoF Relative Camera Pose Estimation</a></papertitle>
              <br>
              <b>Chris Rockwell</b>, 
              <a href="https://nileshkulkarni.github.io/">Nilesh Kulkarni</a>,
              <a href="https://jinlinyi.github.io/">Linyi Jin</a>,
              <a href="https://jjparkcv.github.io/">JJ Park</a>,
              <a href="https://web.eecs.umich.edu/~justincj/">Justin Johnson</a>
              and <a href="https://cs.nyu.edu/~fouhey/">David F. Fouhey</a>
              <br>
              <it>CVPR, 2024 (<b><span style="color:red;"></span>Highlight -- 2.8% accept rate</b>span></b>)</it>
              <br>
              <a href="https://crockwell.github.io/far/">project page</a> /
              <a href="https://arxiv.org/abs/2403.03221">arXiv</a> /
              <a href="https://github.com/crockwell/far/">Github</a> /
              <a href="https://crockwell.github.io/far/rockwell2024.html">bibtex</a> 
              <p>Our flexible method produces accurate and robust pose estimates using complementary strengths of Correspondence + Solver and Learning-Based methods.</p>
              <!--  We leverage complementary strengths of Correspondence + Solver and Learning-Based pose estimation methods to produce accurate and robust pose using a flexible method.</p>-->
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
              <a href="images/Cap3D_thumbnail.PNG"><img src="images/Cap3D_thumbnail.PNG" alt="net1" width="100%"></a>
            </td>
            <td width="65%" valign="middle">
              <papertitle><a href="https://cap3d-um.github.io/">Scalable 3D Captioning with Pretrained Models</a></papertitle>
              <br>
              <a href="https://tiangeluo.github.io/">Tiange Luo*</a>, 
              <b>Chris Rockwell*</b>, 
              <a href="https://web.eecs.umich.edu/~honglak/">Honglak Lee<sup>†</sup></a>
              and <a href="https://web.eecs.umich.edu/~justincj/">Justin Johnson<sup>†</sup></a>
              <br>
              <it>NeurIPS (Datasets and Benchmarks Track) 2023</it>
              <br>
              <a href="https://cap3d-um.github.io/">project page</a> /
              <a href="https://arxiv.org/abs/2306.07279">arXiv</a> /
              <a href="https://github.com/crockwell/Cap3D/">Github</a> /
              <a href="https://crockwell.github.io/Cap3D_luo2023.html">bibtex</a> /
              <a href="https://huggingface.co/datasets/tiange/Cap3D">Hugging Face</a> 
              <!--<a href="https://crockwell.github.io/rel_pose">project page</a> / 
              <a href="https://crockwell.github.io/rel_pose/data/paper.pdf">PDF</a> / 
              <a href="https://github.com/crockwell/rel_pose">code</a> /
              <a href="https://crockwell.github.io/rel_pose/rockwell2022.html">bibtex</a> 
              -->
              <p>Pretrained models are highly effective at 3D captioning; we use them to collect large-scale 3D-text data and finetune text-to-3D models.</p>
              <!--<div class="repo-card" data-repo="crockwell/Cap3D"></div>-->
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
              <a href="images/rel_pose_thumbnail.png"><img src="images/rel_pose_thumbnail.png" alt="net1" width="100%"></a>
            </td>
            <td width="65%" valign="middle">
              <papertitle><a href="https://crockwell.github.io/rel_pose">The 8-Point Algorithm as an Inductive Bias for Relative Pose Prediction by ViTs</a></papertitle>
              <br>
              <b>Chris Rockwell</b>, 
              <a href="https://web.eecs.umich.edu/~justincj/">Justin Johnson</a>
              and <a href="https://cs.nyu.edu/~fouhey/">David F. Fouhey</a>
              <br>
              <it>3DV 2022</it>
              <br>
              <a href="https://crockwell.github.io/rel_pose">project page</a> / 
              <a href="https://crockwell.github.io/rel_pose/data/paper.pdf">PDF</a> / 
              <a href="https://github.com/crockwell/rel_pose/">Github</a> /
              <a href="https://crockwell.github.io/rel_pose/rockwell2022.html">bibtex</a> 
              <!--<a href="https://crockwell.github.io/rel_pose">project page</a> / 
              <a href="https://crockwell.github.io/rel_pose/data/paper.pdf">PDF</a> / 
              <a href="https://github.com/crockwell/rel_pose">code</a> /
              <a href="https://crockwell.github.io/rel_pose/rockwell2022.html">bibtex</a> 
              -->
              <p>Small modifications to a ViT enable computations 
                similar to the Eight-Point algorithm, 
                and are a good inductive bias for pose estimation.</p>
              <!--<div class="repo-card" data-repo="crockwell/rel_pose"></div>-->
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
              <a href="images/planeformers_thumbnail.jpg"><img src="images/planeformers_thumbnail.jpg" alt="net1" width="100%"></a>
            </td>
            <td width="65%" valign="middle">
              <papertitle><a href="https://samiragarwala.github.io/PlaneFormers/">PlaneFormers: From Sparse View Planes to 3D Reconstruction</a></papertitle>
              <br>
              <a href="https://github.com/samiragarwala">Samir Agarwala</a>,
              <a href="https://jinlinyi.github.io/">Linyi Jin</a>,
              <b>Chris Rockwell</b> and 
              <a href="https://cs.nyu.edu/~fouhey/">David F. Fouhey</a>
              <br>
              <it>ECCV 2022</it>
              <br>
              <!--
              
              <a href="">video</a> /-->
              <a href="https://samiragarwala.github.io/PlaneFormers/">project page</a> / 
              <a href="https://arxiv.org/pdf/2208.04307.pdf">arXiv</a> / 
              <a href="https://github.com/samiragarwala/PlaneFormers">Github</a> /
              <a href="https://crockwell.github.io/agarwala2022.html">bibtex</a> 
              <p>Transformers are really good at integrating evidence across multiple views and producing a planar reconstruction.
              </p>
              <!--<div class="repo-card" data-repo="samiragarwala/PlaneFormers"></div>-->
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
              <a href="images/FWD_model.png"><img src="images/FWD_model.png" alt="net1" width="100%"></a>
            </td>
            <td width="65%" valign="middle">
              <papertitle><a href="https://caoang327.github.io/FWD/">FWD: Real-time Novel View Synthesis with Forward Warping and Depth</a></papertitle>
              <br>
              <a href="https://caoang327.github.io/">Ang Cao</a>,
              <b>Chris Rockwell</b> and 
              <a href="https://web.eecs.umich.edu/~justincj">Justin Johnson</a>
              <br>
              <it>CVPR 2022</it>
              <br>
              <a href="https://caoang327.github.io/FWD/">project page</a> / 
              <a href="https://arxiv.org/pdf/2206.08355.pdf">arXiv</a> / 
              <a href="https://github.com/Caoang327/fwd_code">Github</a> /
              <a href="https://crockwell.github.io/cao2022.html">bibtex</a> / 
              <a href="https://www.youtube.com/watch?v=d7m1fJ1LcAY&ab_channel=AngCao">video</a>
              <p>Our forward-warping-based method enables real-time, high-quality novel view synthesis on novel objects from sparse views.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
              <a href="images/articulation.png"><img src="images/articulation.png" alt="net1" width="100%"></a>
            </td>
            <td width="65%" valign="middle">
              <papertitle><a href="https://jasonqsy.github.io/Articulation3D/">Understanding 3D Object Articulation in Internet Videos</a></papertitle>
              <br>
              <a href="https://jasonqsy.github.io">Shengyi Qian</a>,
              <a href="https://jinlinyi.github.io/">Linyi Jin</a>,
              <b>Chris Rockwell</b>, 
              <a href="https://chicychen.github.io/">Siyi Chen</a> and
              <a href="https://cs.nyu.edu/~fouhey/">David F. Fouhey</a>
              <br>
              <it>CVPR 2022</it>
              <br>
              <a href="https://jasonqsy.github.io/Articulation3D/">project page</a> / 
              <a href="https://arxiv.org/abs/2203.16531">arXiv</a> / 
              <a href="https://github.com/JasonQSY/Articulation3D">Github</a> /
              <a href="https://cs.nyu.edu/~fouhey/2022/articulation/qian22.bib">bibtex</a> / 
              <a href="https://www.youtube.com/watch?v=yUptXNEMS6g">video</a>
              <p> By training on both video data and 3D reconstructions in the right way, we can build models of articulations of 3D objects on ordinary video data.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
              <a href="images/pixelsynth_teaser.png"><img src="images/pixelsynth_teaser.png" alt="net1" width="100%"></a>
            </td>
            <td width="65%" valign="middle">
              <papertitle><a href="https://crockwell.github.io/pixelsynth">PixelSynth: Generating a 3D-Consistent Experience from a Single Image</a></papertitle>
              <br>
              <b>Chris Rockwell</b>, <a href="https://cs.nyu.edu/~fouhey/">David F. Fouhey</a> and <a href="https://web.eecs.umich.edu/~justincj/">Justin Johnson</a>
              <br>
              <it>ICCV 2021</it>
              <br>
              <a href="https://crockwell.github.io/pixelsynth">project page</a> / 
              <a href="https://crockwell.github.io/pixelsynth/data/paper.pdf">PDF</a> / 
              <a href="https://github.com/crockwell/pixelsynth">Github</a> /
              <a href="https://crockwell.github.io/pixelsynth/rockwell2021.html">bibtex</a> / 
              <a href="https://cse.engin.umich.edu/stories/generating-3d-spaces-from-a-single-picture">press</a>
              <p> Combining 3D reasoning and autoregressive modeling facilitates high-quality and consistent single-image novel view synthesis.
              <!--<p> PixelSynth fuses the complementary strengths of 
                3D reasoning and autoregressive modeling to 
                create an immersive experience from a single image.-->
              </p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
              <a href="images/humans_teaser.png"><img src="images/humans_teaser.png" alt="net1" width="100%"></a>
            </td>
            <td width="65%" valign="middle">
              <papertitle><a href="https://crockwell.github.io/partial_humans">Full-Body Awareness from Partial Observations</a></papertitle>
              <br>
              <b>Chris Rockwell</b> and <a href="https://cs.nyu.edu/~fouhey/">David F. Fouhey</a>
              <br>
              <it>ECCV 2020</it>
              <br>
              <a href="https://crockwell.github.io/partial_humans">project page</a> / 
              <a href="https://crockwell.github.io/partial_humans/data/paper.pdf">PDF</a> / 
              <a href="https://github.com/crockwell/partial_humans">Github</a> /
              <a href="https://cs.nyu.edu/~fouhey/2020/partialhumans/rockwell2020.bib">bibtex</a> / 
              <a href="https://cse.engin.umich.edu/stories/new-research-teaches-ai-how-people-move-with-internet-videos">press</a>
              <p> A simple self-training framework significantly improves 3D human pose estimation on highly-truncated Internet videos.
              </p>
            </td>
          </tr>
        </tbody></table>
        
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Open-Sourced Projects</heading>
            </td>
          </tr>
        </tbody></table>
          
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1603.06937"><img src="images/hourglass2.PNG" alt="net1" width="100%"></a>
            </td>
            <td width="65%" valign="middle">
              <!--
              <papertitle><a href="https://github.com/princeton-vl/pytorch_stacked_hourglass">Stacked Hourglass Networks for Human Pose Estimation</a></papertitle>
              <br><b>Chris Rockwell</b>, <a href="https://www.alejandronewell.com/">Alejandro Newell</a> and <a href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a>
              <br>Graduate Research, Princeton Vision and Learning Lab, 2019.
              <br><a href="https://github.com/princeton-vl/pytorch_stacked_hourglass">Github</a>
              <br><p> We implement and replicate results from <em>Newell et al.</em>'s <a href="https://arxiv.org/pdf/1603.06937.pdf">Stacked Hourglass Networks</a> in PyTorch.
              </p>
              -->
              <div class="repo-card" data-repo="princeton-vl/pytorch_stacked_hourglass"></div>
            </td>
            
          </tr>

        <!--
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Projects</heading>
            </td>
          </tr>
        </tbody></table>
          

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
              <a href="images/meta-dataset.JPG"><img src="images/meta-dataset.JPG" alt="meta" width="100%"></a>
            </td>
            <td width="65%" valign="middle">
              <papertitle>A Simple Baseline on Meta-Dataset</papertitle>
              <br>
              Graduate Research, Princeton Vision and Learning Lab. Spring 2019.
              <br>
              Advisor: <a href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a>
              <br>  
              <p> We improve a simple fine-tuning baseline on <a href="https://arxiv.org/pdf/1903.03096.pdf"> Meta-Dataset </a> to rival 
                the SOTA meta-learning based method, 
                using higher regularization on fine-tuning layer compared to the backbone.             
              </p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
              <a href="images/net1_improved_4.JPG"><img src="images/net1_improved_4.JPG" alt="net1" width="100%"></a>
            </td>
            <td width="65%" valign="middle">
              <papertitle><a href="https://github.com/crockwell/pytorch_stacked_hourglass_attention">Hourglass Networks with Top-Down Modulation for Human Pose Estimation</a></papertitle>
              <br>
              Graduate Research, Princeton Vision and Learning Lab. Summer 2018 - Winter 2019.
              <br>
              Advisor: <a href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a>
              <br>  
              <a href="https://github.com/princeton-vl/pytorch_stacked_hourglass">Github: Hourglass Implementation</a> /
              <a href="https://github.com/crockwell/pytorch_stacked_hourglass_attention">Github: Hourglass Extension</a>
              <br>
              <p> We <a href="https://github.com/princeton-vl/pytorch_stacked_hourglass"></a>implement in PyTorch</a> <em> Newell et al.</em>'s <a href="https://arxiv.org/pdf/1603.06937.pdf">Stacked Hourglass Networks</a>,
                and improve performance using a decoder network as attention.
              </p>
            </td>
          </tr>
        -->

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Professional Experience</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
              <img style="display:block;" width="100%" src="images/nvidia.png" alt="nvidia">
            </td>
            <td width="65%" valign="center">
              <papertitle><a href="https://research.nvidia.com/labs/dir/">NVIDIA</a></papertitle>, Spring 2024.
              <br> Research Intern (Deep Imagination Research Group)
              <br> Hosts: <a href="https://chenhsuanlin.bitbucket.io/">Chen-Hsuan Lin</a>, <a href="https://tsungyilin.info/">Tsung-Yi Lin</a>, 
              and <a href="https://mingyuliu.net/">Ming-Yu Liu</a>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
              <img style="display:block;" width="100%" src="images/meta.png" alt="meta">
            </td>
            <td width="65%" valign="center">
              <papertitle><a href="https://about.meta.com/realitylabs/">Meta Reality Labs</a></papertitle>, Summer - Fall 2022.
              <br> Research Scientist Intern (Computational Photography Research Group)
              <br> Hosts: <a href="https://hytseng0509.github.io/">Hung-Yu Tseng</a>, <a href="https://jbhuang0604.github.io/">Jia-Bin Huang</a>, 
              and <a href="http://johanneskopf.de/">Johannes Kopf</a>
            </td>
          </tr>
          <tr>
            <tr>
              <td style="padding:20px;width:35%;vertical-align:middle">
                <img style="display:block;" width="100%" src="images/turingsense.png" alt="turingsense">
              </td>
              <td width="65%" valign="center">
                <papertitle><a href="https://www.turingsense.com/">TuringSense, Inc.</a></papertitle>, Winter - Spring 2021.
                <br> Technical Consultant (Computer Vision).
              </td>
            </tr>
          <tr>
          <!-- 
          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
              <img style="display:block;" width="100%" src="images/um.jpg" alt="um2">
            </td>
            <td width="65%" valign="center">
                EECS 498/598 Deep Learning, Winter 2019: Grader
                <br>
                Instructor: Honglak Lee
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle"><a href="https://blog.supplysideliberal.com/"><img style="display:block;" width="100%" src="images/ssl.JPG"></a></td>
            <td width="65%" valign="center">
              Guest blog post on Miles Kimball's Economics blog <em>Confessions of a Supply Side Liberal</em>
                <br>
                <a href="http://blog.supplysideliberal.com/post/114381051097/chris-rockwell-has-a-masters-degree-become-a">post</a>
            </td>
          </tr>
        -->
          
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Activities</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
              <img style="display:block;" width="100%" src="images/ai4all.jpg" alt="um2">
            </td>
            <td width="65%" valign="center">
              <papertitle><a href="https://ai.engin.umich.edu/michigan-theory-home/ai4all-summer-camp/">AI4ALL</a></papertitle>, Summer 2020 and 2021.
                <br> Project Instructor, Application Reviewer, Curriculum Advisory Board
                <br><a href="https://github.com/crockwell/ai4all_cancer_detection">Project Github</a>
                <br> Michigan AI4ALL Lead: <a href="https://cs.nyu.edu/~fouhey/">David F. Fouhey</a>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
              <img style="display:block;" width="100%" src="images/AURA.jpg" alt="um2">
            </td>
            <td width="65%" valign="center">
              <papertitle><a href="https://aura.engin.umich.edu/">African Undergraduate Research Adventure (AURA)</a></papertitle>, Summer 2020.
              <br> Graduate Research Mentor
              <br> Research Advisor: <a href="https://cs.nyu.edu/~fouhey/">David F. Fouhey</a>
            </td>
          </tr>
          <!-- 
          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
              <img style="display:block;" width="100%" src="images/um.jpg" alt="um2">
            </td>
            <td width="65%" valign="center">
                EECS 498/598 Deep Learning, Winter 2019: Grader
                <br>
                Instructor: Honglak Lee
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle"><a href="https://blog.supplysideliberal.com/"><img style="display:block;" width="100%" src="images/ssl.JPG"></a></td>
            <td width="65%" valign="center">
              Guest blog post on Miles Kimball's Economics blog <em>Confessions of a Supply Side Liberal</em>
                <br>
                <a href="http://blog.supplysideliberal.com/post/114381051097/chris-rockwell-has-a-masters-degree-become-a">post</a>
            </td>
          </tr>
        -->
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p>
                <a href="https://jonbarron.info/">Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
