<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Chris Rockwell</title>
  
  <meta name="author" content="Chris Rockwell">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- for Twitter / facebook sharing -- not implemented yet
  <meta property="og:locale" content="en_US">
  <meta property="og:type" content="website">
  <meta property="og:title" content="Karan Desai">
  <meta property="og:url" content="https://kdexd.github.io/">
  <meta property="og:site_name" content="Karan Desai">
  <meta property="og:image" content="https://kdexd.github.io/static/img/kd.jpg"/>
  <meta property="og:image:url" content="https://kdexd.github.io/static/img/kd.jpg"/>
  <meta property="og:description" content="Visiting research scholar at Georgia Tech, IIT Roorkee Alum.">
  <meta name="twitter:card" content="summary_large_image"/>
  <meta name="twitter:title" content="Karan Desai">
  <meta name="twitter:description" content="Visiting research scholar at Georgia Tech, IIT Roorkee Alum.">
  <meta name="twitter:creator" content="@kdexd">

  <meta name="twitter:image" content="https://kdexd.github.io/static/img/kd.jpg"/>
  <meta name="twitter:url" content="https://kdexd.github.io/static/img/kd.jpg"/>
  <meta name="twitter:description" content="Visiting research scholar at Georgia Tech, IIT Roorkee Alum."/>
  <meta name="twitter:domain" value="kdexd.github.io">

  <meta name="twitter:label1" value="Visiting Research Scholar"/>
  <meta name="twitter:data1" value="Georgia Tech"/>
  -->

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-153849999-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-153849999-1');



  var images = ['images/prof_crop_circle1.png','images/prof_crop_circle2.png','images/prof_crop_circle3.png'], 
    index = 0, // starting index
    maxImages = images.length - 1;
  var timer = setInterval(function() {
    var curImage = images[index];
    index = (index == maxImages) ? 0 : ++index;
    // set your image using the curImageVar 
    $('div.mystuff img').attr('src','images/'+curImage);
  }, 500);
</script>

</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Chris Rockwell</name>
              </p>
              <p>I am a Ph.D. candidate in Computer Science and Engineering at the <a href="https://www.cse.umich.edu/">University of Michigan</a>.
                I do research in computer vision and machine learning and am advised by
                <a href="https://web.eecs.umich.edu/~fouhey/">David F. Fouhey</a> and <a href="https://web.eecs.umich.edu/~justincj/">Justin Johnson</a>.
                <!-- I am currently interning in Computational Photography Research at <a href="https://tech.fb.com/ar-vr/">Meta Reality Labs</a>. 
                My managers are <a href="https://hytseng0509.github.io/">Hung-Yu Tseng</a> and <a href="http://johanneskopf.de/">Johannes Kopf</a>. 
                -->
                <!-- I also obtained my Master's at the University of Michigan, where I was advised by David Fouhey and 
                <a href="https://www.cs.princeton.edu/~jiadeng/">Jia Deng</a>. 
                -->
              </p>
              <!--<p> Before grad school, I worked as a trader at <a href="https://www.citadel.com/">Citadel LLC</a>, where I applied statistical arbitrage in fixed income. Earlier, I structured exotic options and created systematic hedging strategies at <a href="https://group.bnpparibas/en/">BNP Paribas</a>. I received my Bachelor's at the University of Michigan, where I researched High Frequency Trading and AI under <a href="https://strategicreasoning.org/michael-p-wellman/"> Michael Wellman</a>. 
              </p>
              -->
              <p> Feel free to reach out! My email is cnris at umich dot edu
              </p>
              <p style="text-align:center">
                <a href="https://github.com/crockwell">Github</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=pB9hZ_MAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="data/cv.pdf">CV</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/chris-rockwell-54903b65/"> LinkedIn </a> &nbsp/&nbsp
                <a href="https://twitter.com/_crockwell">Twitter</a>
                
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <img style="width:100%;max-width:100%" alt="profile photo" src="images/prof_crop_circle1.png" class="hoverZoomLink">
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Preprints</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
              <a href="images/Cap3D_thumbnail.PNG"><img src="images/Cap3D_thumbnail.PNG" alt="net1" width="100%"></a>
            </td>
            <td width="65%" valign="middle">
              <papertitle><a href="https://arxiv.org/abs/2306.07279">Scalable 3D Captioning with Pretrained Models</a></papertitle>
              <br>
              <a href="https://tiangeluo.github.io/">Tiange Luo*</a>, 
              <b>Chris Rockwell*</b>, 
              <a href="https://web.eecs.umich.edu/~honglak/">Honglak Lee<sup>†</sup></a>
              and <a href="https://web.eecs.umich.edu/~justincj/">Justin Johnson<sup>†</sup></a>
              <br>
              <it>arXiv 2023</it>
              <br>
              <a href="https://cap3d-um.github.io/">project page</a> /
              <a href="https://arxiv.org/abs/2306.07279">arXiv</a> /
              <a href="https://huggingface.co/datasets/tiange/Cap3D">Hugging Face</a> / 
              <a href="https://github.com/crockwell/Cap3D/">Github</a> /
              <a href="https://crockwell.github.io/Cap3D_luo2023.html">bibtex</a> 
              <!--<a href="https://crockwell.github.io/rel_pose">project page</a> / 
              <a href="https://crockwell.github.io/rel_pose/data/paper.pdf">PDF</a> / 
              <a href="https://github.com/crockwell/rel_pose">code</a> /
              <a href="https://crockwell.github.io/rel_pose/rockwell2022.html">bibtex</a> 
              -->
              <p>Pretrained models are highly effective at 3D captioning; we use them to collect large-scale 3D-text data and finetune text-to-3D models.</p>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
              <a href="images/rel_pose_thumbnail.png"><img src="images/rel_pose_thumbnail.png" alt="net1" width="100%"></a>
            </td>
            <td width="65%" valign="middle">
              <papertitle><a href="https://crockwell.github.io/rel_pose">The 8-Point Algorithm as an Inductive Bias for Relative Pose Prediction by ViTs</a></papertitle>
              <br>
              <b>Chris Rockwell</b>, 
              <a href="https://web.eecs.umich.edu/~justincj/">Justin Johnson</a>
              and <a href="https://web.eecs.umich.edu/~fouhey/">David F. Fouhey</a>
              <br>
              <it>3DV 2022</it>
              <br>
              <a href="https://crockwell.github.io/rel_pose">project page</a> / 
              <a href="https://crockwell.github.io/rel_pose/data/paper.pdf">PDF</a> / 
              <a href="https://github.com/crockwell/rel_pose/">Github</a> /
              <a href="https://crockwell.github.io/rel_pose/rockwell2022.html">bibtex</a> 
              <!--<a href="https://crockwell.github.io/rel_pose">project page</a> / 
              <a href="https://crockwell.github.io/rel_pose/data/paper.pdf">PDF</a> / 
              <a href="https://github.com/crockwell/rel_pose">code</a> /
              <a href="https://crockwell.github.io/rel_pose/rockwell2022.html">bibtex</a> 
              -->
              <p>Small modifications to a ViT enable computations 
                similar to the Eight-Point algorithm, 
                and are a good inductive bias for pose estimation.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
              <a href="images/planeformers_thumbnail.jpg"><img src="images/planeformers_thumbnail.jpg" alt="net1" width="100%"></a>
            </td>
            <td width="65%" valign="middle">
              <papertitle><a href="https://github.com/samiragarwala">PlaneFormers: From Sparse View Planes to 3D Reconstruction</a></papertitle>
              <br>
              <a href="https://github.com/samiragarwala">Samir Agarwala</a>,
              <a href="https://jinlinyi.github.io/">Linyi Jin</a>,
              <b>Chris Rockwell</b> and 
              <a href="https://web.eecs.umich.edu/~fouhey/">David F. Fouhey</a>
              <br>
              <it>ECCV 2022</it>
              <br>
              <!--
              
              <a href="">video</a> /-->
              <a href="https://samiragarwala.github.io/PlaneFormers/">project page</a> / 
              <a href="https://arxiv.org/pdf/2208.04307.pdf">arXiv</a> / 
              <a href="https://github.com/samiragarwala/PlaneFormers">Github</a> /
              <a href="https://crockwell.github.io/agarwala2022.html">bibtex</a> 
              <p>Transformers are really good at integrating evidence across multiple views and producing a planar reconstruction.
              </p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
              <a href="images/FWD_model.png"><img src="images/FWD_model.png" alt="net1" width="100%"></a>
            </td>
            <td width="65%" valign="middle">
              <papertitle><a href="https://caoang327.github.io/FWD/">FWD: Real-time Novel View Synthesis with Forward Warping and Depth</a></papertitle>
              <br>
              <a href="https://caoang327.github.io/">Ang Cao</a>,
              <b>Chris Rockwell</b> and 
              <a href="https://web.eecs.umich.edu/~justincj">Justin Johnson</a>
              <br>
              <it>CVPR 2022</it>
              <br>
              <a href="https://caoang327.github.io/FWD/">project page</a> / 
              <a href="https://arxiv.org/pdf/2206.08355.pdf">arXiv</a> / 
              <a href="https://www.youtube.com/watch?v=d7m1fJ1LcAY&ab_channel=AngCao">video</a> /
              <a href="https://github.com/Caoang327/fwd_code">Github</a> 
              <p>Our forward-warping-based method enables real-time, high-quality novel view synthesis on novel objects from sparse views.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
              <a href="images/articulation.png"><img src="images/articulation.png" alt="net1" width="100%"></a>
            </td>
            <td width="65%" valign="middle">
              <papertitle><a href="https://jasonqsy.github.io/Articulation3D/">Understanding 3D Object Articulation in Internet Videos</a></papertitle>
              <br>
              <a href="https://jasonqsy.github.io">Shengyi Qian</a>,
              <a href="https://jinlinyi.github.io/">Linyi Jin</a>,
              <b>Chris Rockwell</b>, 
              <a href="https://chicychen.github.io/">Siyi Chen</a> and
              <a href="https://web.eecs.umich.edu/~fouhey/">David F. Fouhey</a>
              <br>
              <it>CVPR 2022</it>
              <br>
              <a href="https://jasonqsy.github.io/Articulation3D/">project page</a> / 
              <a href="https://arxiv.org/abs/2203.16531">arXiv</a> / 
              <a href="https://github.com/JasonQSY/Articulation3D">Github</a> /
              <a href="https://web.eecs.umich.edu/~fouhey/2022/articulation/qian22.bib">bibtex</a> / 
              <a href="https://www.youtube.com/watch?v=yUptXNEMS6g">CVPR talk</a>
              <p> By training on both video data and 3D reconstructions in the right way, we can build models of articulations of 3D objects on ordinary video data.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
              <a href="images/pixelsynth_teaser.png"><img src="images/pixelsynth_teaser.png" alt="net1" width="100%"></a>
            </td>
            <td width="65%" valign="middle">
              <papertitle><a href="https://crockwell.github.io/pixelsynth">PixelSynth: Generating a 3D-Consistent Experience from a Single Image</a></papertitle>
              <br>
              <b>Chris Rockwell</b>, <a href="https://web.eecs.umich.edu/~fouhey/">David F. Fouhey</a> and <a href="https://web.eecs.umich.edu/~justincj/">Justin Johnson</a>
              <br>
              <it>ICCV 2021</it>
              <br>
              <a href="https://crockwell.github.io/pixelsynth">project page</a> / 
              <a href="https://crockwell.github.io/pixelsynth/data/paper.pdf">PDF</a> / 
              <a href="https://github.com/crockwell/pixelsynth">Github</a> /
              <a href="https://crockwell.github.io/pixelsynth/rockwell2021.html">bibtex</a> / 
              <a href="https://cse.engin.umich.edu/stories/generating-3d-spaces-from-a-single-picture">press</a>
              <p> Combining 3D reasoning and autoregressive modeling facilitates high-quality and consistent single-image novel view synthesis.
              <!--<p> PixelSynth fuses the complementary strengths of 
                3D reasoning and autoregressive modeling to 
                create an immersive experience from a single image.-->
              </p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
              <a href="images/humans_teaser.png"><img src="images/humans_teaser.png" alt="net1" width="100%"></a>
            </td>
            <td width="65%" valign="middle">
              <papertitle><a href="https://crockwell.github.io/partial_humans">Full-Body Awareness from Partial Observations</a></papertitle>
              <br>
              <b>Chris Rockwell</b> and <a href="https://web.eecs.umich.edu/~fouhey/">David F. Fouhey</a>
              <br>
              <it>ECCV 2020</it>
              <br>
              <a href="https://crockwell.github.io/partial_humans">project page</a> / 
              <a href="https://crockwell.github.io/partial_humans/data/paper.pdf">PDF</a> / 
              <a href="https://github.com/crockwell/partial_humans">Github</a> /
              <a href="https://web.eecs.umich.edu/~fouhey/2020/partialhumans/rockwell2020.bib">bibtex</a> / 
              <a href="https://cse.engin.umich.edu/stories/new-research-teaches-ai-how-people-move-with-internet-videos">press</a>
              <p> A simple self-training framework significantly improves 3D human pose estimation on highly-truncated Internet videos.
              </p>
            </td>
          </tr>
        </tbody></table>
        
        
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Projects</heading>
            </td>
          </tr>
        </tbody></table>
          
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
              <a href="images/meta-dataset.JPG"><img src="images/meta-dataset.JPG" alt="meta" width="100%"></a>
            </td>
            <td width="65%" valign="middle">
              <papertitle>A Simple Baseline on Meta-Dataset</papertitle>
              <br>
              Graduate Research, Princeton Vision and Learning Lab. Spring 2019.
              <br>
              Advisor: Jia Deng 
              <br>  
              <p> We improve a simple fine-tuning baseline on <a href="https://arxiv.org/pdf/1903.03096.pdf"> Meta-Dataset </a> to within 0.1 <em>average rank</em> (minimum reportable difference) of the authors' best meta-learning based method using higher regularization on fine-tuning layer compared to the backbone.
              
                <!--Using Logistic Regression or an SVM instead of an FC layer for fine-tuning produce similar results.-->
              
              </p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
              <a href="images/net1_improved_4.JPG"><img src="images/net1_improved_4.JPG" alt="net1" width="100%"></a>
            </td>
            <td width="65%" valign="middle">
              <papertitle>Hourglass Networks with Top-Down Modulation for Human Pose Estimation</papertitle>
              <br>
              Graduate Research, Princeton Vision and Learning Lab. Summer 2018 - Winter 2019.
              <br>
              Advisor: Jia Deng 
              <br>  
              <a href="https://github.com/crockwell/pytorch_stacked_hourglass_attention">code: attention</a> / <a href="http://www-personal.umich.edu/~cnris/attention_2hg/checkpoint.pt">pretrained model, 2HG attention</a>
              <br>
                <a href="https://github.com/crockwell/pytorch_stacked_hourglass_cutout">code: regularization</a> / <a href="http://www-personal.umich.edu/~cnris/regularization_8hg/checkpoint.pt">pretrained model, 8HG regularization</a>
              <p> We increase performance of <em> Newell et al.</em>'s <a href="https://arxiv.org/pdf/1603.06937.pdf">Stacked Hourglass Networks </a> on MPII using a decoder network as attention, along with 
                cutout and vertical flipping.
              
               <!--<br> Using the 2HG model, this improved validation performance 0.4% and improved performance 0.7% when also using cutout and vertical flipping. On the 8HG model, we were also able to improve test performance from 90.9% to 91.3% by simply adding cutout and vertical flipping. Finally, we increased precision of network confidence and explored utilizing confidence for curriculum sampling of tail cases.
                -->
              
                In addition, we improved precision of network confidence and explored utilizing confidence for curriculum sampling of tail cases.
              </p>
            </td>
          </tr>
          
          <!-- 
          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
              <a href="images/AI_pic.JPG"><img src="images/AI_pic.JPG" alt="clean-usnob" style="display:block;" width="100%"></a>
            </td>
            <td width="65%" valign="middle">
              <papertitle>Evaluating Scene Graph-Generated Images using Visual Question Answering</papertitle>
              <br>
              <strong>Chris Rockwell</strong>.
              <br>
              Course project, EECS 692 Advanced AI, Winter 2019.
              <br>
              Instructor: John Laird. 
              <br>  
              <a href="data/AI_report.pdf">presentation</a> / <a href="data/sg_vqa.pdf">report</a>
              <p> I replicate and summarize
                  <a href="https://arxiv.org/pdf/1804.01622.pdf">
                      <papertitle>
                          Image Generation from Scene Graphs
                      </papertitle>
                  </a>
                  , and evaluate replicated generated images, original generated images, and ground truths using VQA.
              </p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
              <a href="images/cnn_pic.JPG"><img src="images/cnn_pic.JPG" alt="clean-usnob" style="display:block;" width="100%"></a>
            </td>
            <td width="65%" valign="middle">
              <papertitle>Replicating and Improving Stacked Hourglass Networks for Human Pose Estimation</papertitle>
              <br>
              <strong>Chris Rockwell</strong>, Uzziel Cortez, Eric Huang.
              <br>
              Course project, EECS 545 Machine Learning, Fall 2018.
              <br>
              Instructor: Clayton Scott. 
              <br>  
              <a href="data/CNN_joint_estimation.pdf">report</a> / <a href="https://github.com/princeton-vl/pytorch_stacked_hourglass">code</a> / <a href="http://www-personal.umich.edu/~cnris/original_8hg/checkpoint.pt">pretrained model, 8HG</a>
              <p> We replicate and improve upon validation accuracy from
                  <a href="https://arxiv.org/pdf/1603.06937.pdf">
                      <papertitle>
                          Stacked Hourglass Networks for Human Pose Estimation
                      </papertitle>
                  </a>
                  using <a href="https://arxiv.org/pdf/1412.6980.pdf">Adam</a> and larger batch size. I led implementation of the project and it was jointly useful for research. My Pytorch implementation is published in the <a href="https://github.com/princeton-vl/pytorch_stacked_hourglass"> princeton-vl Github</a>.
              </p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
              <a href="images/gta.png"><img src="images/gta.png" alt="clean-usnob" style="display:block;" width="100%"></a>
            </td>
            <td width="65%" valign="middle">
              <papertitle>GTA Perception</papertitle>
              <br>
              Richard Higgins, Parth Chopra, <strong>Chris Rockwell</strong>, Sahib Dhanjal, Ung Hee Lee.
              <br>
              Course project, EECS 598/ROB 535 Self-Driving Cars, Fall 2018.
              <br>
              Instructors: Matthew Johnson-Roberson and Ram Vasudevan
              <br>  
              <a href="https://github.com/relh/gta_perception">code</a>
              <p> We finetune a Squeeze and Excitation ResNet classify objects appearing in road-scene images in the <a href="https://arxiv.org/abs/1610.01983"><papertitle>Driving in the Matrix </papertitle></a> dataset. With improved sampling and data augmentation, we finished top 10 in the class. I helped with implementation and improved data augmentation.
              </p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
              <a href="images/data_mining_pic.JPG"><img src="images/data_mining_pic.JPG" alt="clean-usnob" style="display:block;" width="100%"></a>
            </td>
            <td width="65%" valign="middle">
              <papertitle>Link Prediction on the Patent Citation Network</papertitle>
              <br>
              Samuel Chen, David Dang, Robert Macy, <strong>Chris Rockwell</strong>.
              <br>
              Course project, <a href="http://web.eecs.umich.edu/~dkoutra/courses/W19_598/">EECS 598 Advanced Data Mining</a>, Winter 2019.
              <br>
              Instructor: Danai Koutra. 
              <br>  
              <a href="data/data_mining_poster.pdf">poster</a> / <a href="data/LP_patent.pdf">report</a> / <a href="https://github.com/crockwell/SDNE">code</a>
              <p> 
              We compare several methods for link prediction on partitions of the Patent Citation Network dataset for the first time. The very sparse nature of this graph yielded low performance compared to a more typical dataset, but methods performed well in a temporal setting. I led temporal experiments and adapted representation learning method <a href="https://www.kdd.org/kdd2016/papers/files/rfp0191-wangAemb.pdf">SDNE</a> for our task.
              </p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
              <a href="images/LA.JPG"><img src="images/LA.JPG" alt="clean-usnob" style="display:block;" width="100%"></a>
            </td>
            <td width="65%" valign="middle">
              <papertitle>Market Fragmentation and the Latency Arms Race</papertitle>
              <br>
              Undergraduate Research, Strategic Reasoning Group. Summer 2013.
              <br>
              Advisor: Michael Wellman
              <br>  
              <a href="data/SIREU_poster.pdf">poster</a> / <a href="data/SIREU_pres.pdf">presentation</a>
              <br>
              I was featured in a <a href="https://www.youtube.com/watch?v=1-1-UpBoc7w">UMSI Youtube video</a>. 
              <p> I assisted Erik Brinkman in expanding the agent-based latency arbitrage simulation of <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.306.9477&rep=rep1&type=pdf"> Wah and Wellman </a> to model a prisoner's dilemma in the high-frequency trading space.
              </p>
            </td>
          </tr>
          -->
        </tbody></table>
        
          
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Activities</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
              <img style="display:block;" width="100%" src="images/ai4all.jpg" alt="um2">
            </td>
            <td width="65%" valign="center">
                <a href="https://cedo.engin.umich.edu/ai4all/">AI4ALL</a>, Summer 2020: Project Instructor
                <br> Summer 2021: Application Reviewer, Curriculum Advisory Board
                <br>
                Michigan AI4ALL Lead: David F. Fouhey
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
              <img style="display:block;" width="100%" src="images/AURA.jpg" alt="um2">
            </td>
            <td width="65%" valign="center">
               <a href="https://aura.engin.umich.edu/">African Undergraduate Research Adventure (AURA)</a>, Summer 2020: Research Mentor
                <br>
                Research Advisor: David F. Fouhey
            </td>
          </tr>
          <!-- 
          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle">
              <img style="display:block;" width="100%" src="images/um.jpg" alt="um2">
            </td>
            <td width="65%" valign="center">
                EECS 498/598 Deep Learning, Winter 2019: Grader
                <br>
                Instructor: Honglak Lee
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:35%;vertical-align:middle"><a href="https://blog.supplysideliberal.com/"><img style="display:block;" width="100%" src="images/ssl.JPG"></a></td>
            <td width="65%" valign="center">
              Guest blog post on Miles Kimball's Economics blog <em>Confessions of a Supply Side Liberal</em>
                <br>
                <a href="http://blog.supplysideliberal.com/post/114381051097/chris-rockwell-has-a-masters-degree-become-a">post</a>
            </td>
          </tr>
        -->
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p>
                <a href="https://jonbarron.info/">Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
